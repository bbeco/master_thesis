\chapter{State of the Art}
\lhead{\chaptername~\thechapter. \emph{State of the Art}}
\label{ch:state_of_the_art}
In this chapter we give a formal definition of the VO problem, we describe a 
general perspective SfM pipeline, then 
we explain the fundamental challenges issued by the employment of full 
spherical cameras for VO.

\section{VO Problem}
\label{sec:vo_problem}
As we described in the previous chapter, the VO's goal is to recover the 
camera trajectory while it is moving in the environment. We now
introduce the notations we are going to use for the rest of this work; 
these are the same ones used by Scaramuzza in \cite{scaramuzzaVisualOdometryI}.
Let first assume time is sampled in a sequence of time instants \(k\); 
\(I_{0:n} \) is the set of input frames, with \(I_{k}\) the picture taken by 
the camera at time
\(k\). We define \(C_{0:n}\) as the set of camera 
positions such that \(C_k\) is the position at the \(k\)-th instant.
If we call \(T_k\) the rigid body transformation of the camera between the two
consecutive time instant $k-1$ and $k$, then we have the following relation:
\begin{equation}
	\label{eq:motion_composition}
C_n = C_{n-1} T_n
\end{equation}
\noindent with \(T_k\) defined as
\begin{equation*}
	T_k =
	\begin{bmatrix}
	R_k & \myvec{t}_k \\
	0 & 1
	\end{bmatrix} \text{.}
\end{equation*}
\noindent $R_k$ is a 3-by-3 rotation matrix while $\myvec{t}_k$ is a column vector, 
they represent the rotation and translation the camera performed from instant 
$k-1$ to $k$.
Therefore the goal of VO is to estimate both $R_k$ and $t_k$ for each instant 
$k$ and then compute the camera position $C_k$ accordingly to 
equation~\ref{eq:motion_composition}.
The initial position $C_0$ can be set arbitrarily.

Equation~\ref{eq:motion_composition} is the core of VO: thanks to it we are able
to compute the camera local movement, therefore we can estimate its location in 
every moment. But the equation above contains also the biggest practical 
challenge of VO, that is the error accumulation phenomenon known as 
\textit{drift}.
Every new position $C_k$ introduces an error factor that affects the next 
computations; the results is a global error that grows as the number of 
estimations increases.
All the efforts of the VO research community since early works like 
\cite{harris19883d} and \cite{moravec1980obstacle}, has focused on error
minimization techniques to reduce drift.
These tackle the precise estimation problem either by employing more accurate
local motion estimation and by introducing an optimization step to refine 
camera locations.
The set of techniques by which the drift is reduced once the camera poses have 
have been computed go under the general term \textit{bundle adjustment}.

\section{Perspective SfM}
The SfM literature is extensive but most of the approaches the 
researches followed so far present pipelines similar to the one in 
Fig~\ref{fig:block_diagram}.
The main steps of the : compute the relative motion for each image
pair, then compose these motions to obtain the absolute camera position and 
orientation. Finally, run an optimization procedure to reduce drift.
The differences are in the type of input data, motion estimation algorithm,
optimization procedure and additional constraints considered.
For generic SfM researches, the input data is a set of traditional pictures of 
the same environment from different point of view. The pictures can be taken by 
different cameras with unknown parameters and in different time instants.
\begin{figure}
	\label{fig:block_diagram}
   \centering
    \def\svgwidth{0.5\columnwidth}
    \input{img/block_diagram.pdf_tex}
    \caption{Perspective SfM pipeline block diagram.}
\end{figure}
\textit{Visual Odometry} is based on SfM techniques, 
in this case, the input data is usually a sequence 
of images from a video stream. Many studies targeted different hardware setup 
but the specific image capturing device is usually either a single perspective 
camera or a stereo imaging rig. The computer vision literature refers to the
former case with the term \textit{monocular VO} while it uses \textit{stereo VO}
to describe the latter.

Even though perspective cameras have been the first choice in many studies, 
the researchers used other devices, like the ones described in 
\ref{sec:cameraclassification}.

We use the terms \textit{perspective} or \textit{traditional SfM} to describe 
SfM pipelines designed for perspective cameras.

A great resource about the state of the art for Visual Odometry and Structure 
from Motion techniques is the couple of articles by Scaramuzza,  
\cite{scaramuzzaVisualOdometryI} and \cite{scaramuzzaVisualOdometryII}.

\subsection{Motion Estimation}
We use the term local motion to indicate the camera movement between two 
consecutive images while, on the other hand, we define global motion as
the overall camera's path.
The local motion estimation step is fundamental in a SfM pipeline, its goal is 
to find the rigid body transformation composed of $R_k$ and $t_k$ as we 
described in Section~\ref{sec:vo_problem}.
All the local motion estimation methods described in the literature so far are 
based on the correspondences found in two consecutive images but, depending on the
specific camera rig (stereo imaging systems or monocular), the type of 
correspondences and their matching procedure, we may have several choices for
the actual way to estimate local motion.
There are two main families of point matching algorithms: \textit{feature-based}
and \textit{appearance-based} (also known as \textit{global-methods}).
While the former ones utilize repeatable features 
matching between the images, the latter rely on pixel's intensity information; 
they are simpler but also slower. Most of the 
most recent VO pipelines use feature-based methods because of
their speed and robustness.
A VO implementation that employs intensity based techniques is 
\cite{nister2004visual}, while ...\todo{aggiungere qualche citazione che non usi approcci misti}

Each of the feature points found by the motion estimation phase can be 
either a 3D world-point feature or a 2D image-point one,
therefore, there are three possible combinations that provides just as many 
different matching and motion-estimation procedures:
\begin{itemize}
	\item 2D-to-2D: both feature sets are composed of image points.
The matching metric can be a simple euclidean distance between feature 
descriptors and the motion estimation can be solved by estimating the 
\textit{essential matrix} (see section~\ref{subsec:essential_matrix} for 
details);
	\item 3D-to-3D: both feature sets contain world-points features and motion
estimation is performed by solving an alignment problem;
	\item 3D-to-2D: the previous image's feature set is composed of world points while
the current one's feature set contains their projections. In this case, the motion 
is estimated by solving a \textit{PnP} problem.
\end{itemize}
Since the second and third approaches deal with 3D points, they are usually 
best suited for stereo rig (especially the 3D-to-3D case).
Yet we can still use the 3D-to-2D approach in the monocular VO case by simply 
triangulating corresponding 2D image features in consecutive frames.
In fact, in order to obtain the first 3D feature set,
we can compute the relative motion between the first two images with 
the 2D-to-2D techniques and then obtain the rest of the camera's path,
as we said, by solving the \textit{PnP} problem.

\subsubsection{Essential Matrix}
\label{subsec:essential_matrix}
The essential matrix $E$ is defined by the equation
\begin{equation}
\label{eq:epipolar_equation}
\mathbf{p}'^\top E \mathbf{p} = 0
\end{equation}
\noindent where $\mathbf{p}$ and $\mathbf{p}'$ are the normalized corresponding 
feature coordinates in the image $I_{k-1}$ and $I_{k}$ respectively.
The normalized coordinates are defined as:
\begin{equation}
	\mathbf{p} = K^{-1} \mathbf{m}
\end{equation}
\noindent where $K$ is the intrinsic parameter matrix and $\mathbf{m}$ is an
image point.
The essential matrix contains the geometric information that describe the 
relative location of a camera respect to another one up to an unknown scale factor 
for the translation vector. In particular, we have
\begin{equation*}
	E_k = \lambda \hat{t}_kR_k
\end{equation*}
\noindent with $\lambda$ that is the unknown scale factor and $\hat{t}_k$ is 
the skew-symmetric form for the cross product of vector $t_k$.
In order to extract the local motion given the two set of corresponding features
in two consecutive images, we have to estimate the essential matrix and then 
extract the rigid body transformation out of it.
For the $E$ estimation part we can employ the Longuet-Higgins' 8-points 
algorithm \cite{longuet1981computer}: the equation~\ref{eq:epipolar_equation} 
provides a constraint we
can exploit for computation, in fact, for each correspondence, we can rewrite 
the equation as
\begin{equation*}
	\begin{bmatrix}
		p_1p'_1 & p'_1p_2 & p'_1 & p_1p'_2 & p_2p'_2 & p'_2 & p_1 & p_2 & 1
	\end{bmatrix}
	E = 0	\text{.}
\end{equation*}
To find the nine unknowns we need 8 noncoplanar feature matches that provide as many 
independent equations.
In practice we use more than 8 points, therefore we obtain a overdetermined 
system we solve in the least square sense.

Once $E$ has been estimated, there are 4 possible combinations for the relative 
position of the two cameras and each world points. We are interested in the one
that presents both cameras facing toward the triangulated world point,
i.e. this point has to be in front of both cameras. We can easily chose the 
correct configuration among the four by testing each of them.

\subsubsection{Relative Scale}
As we have already pointed out in the previous section, every monocular SfM 
pipeline that works with 2D-to-2D feature correspondences can estimate the 
local motion up to an unknown scale factor. In fact, there is no way to extract 
the translation magnitude from two sets of features (it is still possible 
to recover the reconstruction scale if we are given some world measure of the
environment).
Therefore, if we can not derive the local translation length with some other
non visual techniques (like wheel odometry, GPS, accelerometer, etc.), 
all we can do is to leave the unknown scale for the motion estimation and 
environment reconstruction.

On the other hand, if we have to compose the motion of more than two poses, 
like VO does, we need to set the first local translation magnitude 
arbitrarily and express all the other movements relative to this first one.
This means that we need to compute the \textit{relative scale}
for every translation other than the first one.

A possible approach for relative scale estimation is described in 
\cite{scaramuzzaVisualOdometryI}. If we want to estimate the relative scale for 
the motion $t_k$, between instants $k-1$ and $k$, we need to compute the 
two sets of triangulated world points $X_{k-1}$ and $X_{k}$ by using 
the two image pairs $I_{k-2}$ and $I_{k-1}$ , and  $I_{k-1}$ and  $I_{k}$ first; then,
given $i$ and $j$, two world points that belongs to both sets $X_{k-1}$ and 
$X_{k}$, we can compute the relative scale accordingly to these points with
\begin{equation}
	\label{eq:relative_scale}
	r = \frac{\| X_{k-1, i} - X_{k - 1, j} \|}{\| X_{k, i} - X_{k, j} \|}
	\text{.}
\end{equation}
\noindent For each world points pair, we obtain a different values of $r$;
we can then select the average of $r$ (or better, its median in case of 
outliers).

It is worth noticing that the exact \textit{absolute scale} for the whole 
camera motion and environment reconstruction is still left unknown.
\todo{aggiungere citazioni}

\section{Bundle Adjustment}
We have already introduced the error accumulation problem that affects SfM in 
section~\ref{sec:vo_problem}; because of the unavoidable errors we introduce 
for each camera pose estimation, an SfM pipeline incorporates a 
refinement process for both camera poses and triangulated points.
This step is called \textit{bundle adjustment} (BA) and its goal is to 
minimize the \textit{reprojection error}.
Point reprojection is the function that projects a world point to image space; 
in particular, the reprojection ${\myvec{\bar{m}}_i}^j$ of the scene point 
$\myvec{M}^j$ according to camera $i$ is defined as
\begin{equation*}
	{\myvec{\bar{m}}_i}^j = K_i[R_i | \myvec{t}_i]\myvec{M}^j	\text{.}
\end{equation*}
\noindent Bundle adjustment can then be expressed as the following minimization 
problem
\begin{equation*}
	\min_{R_i, \myvec{t}_i, \myvec{M}^j} 
	\sum_{i = 1}^N\sum_{j = 1}^n 
	d(K_i[R_i | \myvec{t}_i]\myvec{M}^j, \myvec{m}_i^j) \text{,}
\end{equation*}
\noindent with $\myvec{m}_i^j$ that is the image point of the $i$-th camera
corresponding to $\myvec{M}^j$.
This non linear least squares minimization is usually solved with the 
Levenbergâ€“Marquardt method 
\cite{triggs1999bundle,Hartley2004,levenberg1944method}.
The term \textit{bundle} refers to the fact that both the camera position and
the triangulated points are jointly refined. Most BA implementations keep the 
camera poses fixed, solve for the triangulated points and then optimize for the
poses in an alternate fashion until the desired precision is reached.

BA is a delicate step that can affect an SfM pipeline in many ways
\cite{lourakis2009sba,triggs1999bundle,Hartley2004} as a poorly planned use of 
it can reduce computation speed and fail to obtain the desired accuracy.

\section{Full Spherical Cameras}
\subsection{Image Format}
If we consider the simplest camera model, 
a perspective image is the result of the intersection between the image plane 
and all the light rays that go from the environment to the center of 
projection inside the camera (see Fig\ref{fig:perscam_model}).
\missingfigure{aggiungere modello camera prospettica}
\label{fig:perscam_model}
On the other hand, the full spherical camera model implies the image plane 
to be the result of the intersection between a sphere and the rays from the 
environment to the center of projection (which is equivalent to the sphere's 
center).

