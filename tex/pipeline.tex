\chapter{Spherical SfM}
\lhead{\chaptername~\thechapter. \emph{Spherical SfM}}
In this chapter, we describe the pipeline that we designed to create a dense 
point cloud from a set of equirectangular images.
In Section~\ref{sec:pipeline_pose_estimation}, we describe the first phase of our
pipeline; this estimates selects the frame from the input sequence and estimate
the camera trajectory. In Section~\ref{sec:pipeline_densification}, we describe 
our densification algorithm for equirectangular images.
Fig.~\ref{fig:pipeline_overview} shows a coarse visualization of these two macro parts of the 
pipeline. Pose estimation and densification are 
expanded further in Figures~\ref{fig:sfm_block} and \todo{aggiungere ref a immagine}.

\begin{figure}
    \centering
    \includegraphics[width=0.8\linewidth]{img/pipeline_overview.pdf}
    \caption{The SfM pipeline: the frame selector chooses which frames are 
    relevant for the next steps; the pose estimation phase returns the cameras' 
    poses and the sparse points cloud; the densification step uses 
    the input images and the previous camera's poses to generate a dense
    point cloud. See Figures\ref{fig:sfm_block} and ....}
	\label{fig:pipeline_overview}
\end{figure}
\todo{aggiungere ref a immagine}

\section{Pose Estimation}
\label{sec:pipeline_pose_estimation}
The cameras' poses estimation phase is similar to the classical visual 
odometry pipeline. The main differences lie in the variables' format and in 
the details concerning the procedures used.
For each new frame (in the equirectangular format), we locate the
SURF keypoints \cite{bay2006surf}; we consider those keypoints whose inclination angle is in the range $[-60\degree; +60\degree]$. This is because the poles may be affected by large 
distortions, thus robust matches outside this interval are rare.
Then, we look for matches in the last two frames. A filter performs a 
statistical analysis on the correspondences found and decides whether the frame 
is suitable for a robust pose estimation or not (in this case the frame is 
discarded).

If the frame is kept, its matches with the previous ones are converted
and used for the essential matrix, $E$, estimation. Once $E$ is computed, 
it can be decomposed into the \([R|t]\) form; where $R$ is a rotation matrix 
and $\myvec{t}$ is a translation vector up to an unknown scale factor.

The relative scale can be estimated by using world points that have been 
triangulated through matches and the frame considered in the previous pipeline 
iteration.

Then, the pose of each camera can be described as the composition of the 
motions between each view pair. We perform a bundle adjustment for the last five 
poses in order to reduce the effect of drift.
\textbf{Francesco's note: metti un puntatore al drift definito nel capitolo precedente}

After the processing of all images, a final bundle adjustment step is performed. 
This optimizes every camera pose (previously estimated) in order to reduce drift error further.

Figure~\ref{fig:camera_model} shows the coordinate system used in our pipeline: the X-axis points right, the Y-axis points down, and the Z-axis points forward. We consider the spherical image divided into two hemispheres: front and back. We call \textit{frontal points} the image points whose 3rd component is positive, otherwise we use the term \textit{rear points}.
When we need to express spherical coordinates, we use the same angles as shown in 
Figure~\ref{fig:camera_model}:
the angle between the Z-axis and the projection of $m$ on the XZ-plane in 
the clockwise direction is the longitude angle ($\lambda$), and the angle 
between the negative Y-axis and the same projection of $m$ is the latitude 
angle ($\phi$).

\textbf{Francesco's note: il flow chart non ha un begin vero, martedi' ti spiego come cambiare il flow chart}
\begin{figure}
	\centering
	\includegraphics[width=0.5\linewidth]{img/sfm_block.pdf}
	\caption{The flow chart of steps performed in the pose estimation 
	phase of our SfM pipeline.}
	\label{fig:sfm_block}
\end{figure}

\subsection{Keypoints and Features Extraction}
In Section~\ref{sec:pipeline_pose_estimation}, we have introduced the need for 
features detection in order to find correspondences among the image pairs.
In our pipeline, we use SURF features and descriptors 
\cite{bay2006surf}, which is a blob detector partly inspired by SIFT
\cite{lowe1999object}. SURF performs convolution on integral images with box filters in order to 
compute the Hessian matrix in scale space.
This descriptor exploit keypoint's neighbourhood response to the Haar wavelet.
SURF, as SIFT, is patented. However, it can be used freely in non-commercial
applications and for academic research.
An implementation of the SURF detector is available in the MATLAB's Computer 
Vision Toolbox, and we employed it in our pipeline.

\subsection{Keypoints Filtering}
The equirectangular format for spherical images introduces significant 
distortions around poles. Keypoints in those areas 
are unlikely to match reliably with other points in a consecutive view, therefore they are discarded.
Besides, in our experiments setup, the North and South poles typically point 
upward and downward respectively, while most of the matches useful for pose 
estimation comes from the sides of the camera. Therefore, the removal of interesting 
points near the poles does not affect the final result.

\subsection{Features Matching}
\textbf{Francesco's note: aggiungere un \\ref alla sezione nello stato dell'arte dove questa strategia
viene descritta.}
The matching strategy, which we carried out for equirectangular images, is the same one 
adopted with standard images: two keypoints matches if the distance between 
their descriptors is less than a given threshold. Ambiguous matches 
are discarded if the ratio between the distances to the two closest matches is 
above a maximum.
We enforce matching robustness by forcing unique correspondences; i.e.,
only one feature in the first image can match with another one in the second
image. This is achieved with two passes of the matching procedure. During the 
first pass, each feature in the first images is checked for correspondences in
the second image. In the second pass, each feature of the second image that 
has at least one correspondence in the first image is checked to see what 
interesting point of the first image it corresponds to and the match with the
highest confidence is kept.

\subsection{Matches Filter}
In this step of the pipeline, matches are analyzed in order to decide 
whether a frame has to be kept because it is useful for motion estimation or
not.
%
We assume that the scene to capture is static; i.e., no moving objects (e.g., cars) or people. In this case, the apparent movement of corresponding objects in different views is caused by the camera motion only.
%
The filter computes the disparity between every match found in two views; i.e., 
it compares the median of the 20\% of matches with the highest 
disparity value with a threshold. If such median is above 
the threshold, the frame is kept for further processing in the pipeline, 
otherwise it is discarded.
%
The reason why we consider the correspondences with the greatest disparity is 
because, when the camera rotation is limited, the points that move very little 
in consecutive views are typically far. The selection of these points for motion 
estimation can be counter-productive because they can easily produce numerical 
errors. 

\subsection{3D-to-2D Keypoints Conversion}
\label{sec:keypoints_conversion}
%
This step converts the keypoint format of equirectangular images to a new one, which is
suitable for estimating $E$.
%
First, the latitude and longitude coordinates of each feature point are 
extracted from the 2D mapping according to Equation~\ref{eq:ll2Cartesian_first}.
This equation returns spherical coordinates for each feature point. 
Then, we can convert each of them to its cartesian format using Equation~\ref{eq:ll2Cartesian_second}. 
%
In order to estimate $E$ for each view pair, we use Equation~\ref{eq:epipolar_equation}, which was
introduced by Longuet-Higgins~\cite{longuet1981computer}.
The point coordinates, which we obtain from Equation~\ref{eq:ll2Cartesian_first} and
Equation~\ref{eq:ll2Cartesian_second}, do not need normalisation becuase there
is no intrinsic parameter for the full spherical camera model.

In order to exploit functions to estimate $E$ available in the MATLAB's Computer Vision Toolbox, we need to perform an additional straightforward step.
%
Indeed this toolbox's routines deal with 2D perspective images. Therefore, they expect 
2D vectors when the input arguments are image points. However, our camera provides 3D image points.
We noticed that we can multiply the points $\myvec{p}$ and $\myvec{p}'$ by two 
scalars ${\lambda}$ and ${\lambda}'$ and the equation is still valid. Thus, we obtain

\begin{equation*}
\lambda^\prime{\mathbf{p}^\prime}^\top E\lambda\mathbf{p} = 0 \text{.}
\end{equation*}

Therefore, we divide the 3D points obtained from the spherical images by their 
3rd component, discard it, and use the resulted 2D points as input for the 
MATLAB's function {\tt estimateEssentialMatrix}, which estimates $E$.

\begin{figure}
    \centering
    \def\svgwidth{0.8\columnwidth}
    \input{img/featurepoints_conversion.pdf_tex}
    \caption{A top view representation of the full spherical image's 
    different portions.
    The feature points that lie on the blue and green parts of the sphere are kept and,
    after the conversion described in Section~\ref{sec:keypoints_conversion},
    they are used for $E$ estimation. On the other hand, the points that lie
    on the red portions of the sphere are just discarded.
    Both the frontal point $p$ and the rear point $q$ are projected in $m$.}
	\label{fig:sphere_division}
\end{figure}

\subsubsection{Division by 3rd component vs. Projection}
%Our method (division by the 3rd component of the vectors) and the perspective projection on a plane for feature points conversion have some important differences.
%
Kangni et al.\cite{kangni2007orientation} converted the 
feature points extracted from spherical images to their projected images on a 
planar surface and used traditional techniques to estimate the camera poses.
%
Even though our method is very similar to Kangni et al., there are some differences.
\textbf{Francesco's note: CHE DIFFERENZE???}
Projecting a point means that we have to deal with perspective geometry and its
parameters (e.g., pixel size or density), the principal point coordinates, 
focal length, image size, etc.

We can think to our method as a simplified perspective projection in which
$f_x$ and $f_y$ are both set to 1, and $u_0$ and $v_0$ are 0.
The main difference between our method and a standard projection is that, if we
just divide each feature point by its 3rd component, we do not need to 
differentiate between frontal and rear points.
Figure~\ref{fig:sphere_division} shows how both the frontal point, $p$, and the 
rear one, $q$, are projected to the same point $m$ on the image plane.
This is not an issue, on the contrary, this helps the estimation of $E$ because 
it adds more redundant data to the input. We would obtain the
same $E$ estimation, if we used frontal/rear points only.

Note that we need to take care of numerical errors only: if we divide by a 
small number, the result is affected by a large error. Therefore, we
set a minimum value for the 3rd component magnitude a feature point must 
have in order for it to be used in motion estimation. \textbf{<---Francesco's note: non e' chiara questa ultima frase.}
%
We called this threshold parameter $z_{min}$.
In Figure~\ref{fig:sphere_division}, the red area is not taken into account 
during motion estimation because the value of the 3rd component magnitude of points there 
is below $z_{min}$.
%
Selecting points whose last component is above a certain threshold is 
equivalent to discarding those points that do not fit in the image plane
when projecting them. More details on this theory can be found in the Szeliski's book\cite{szeliski2010computer} and Hartley and Zisserman's books\cite{Hartley2004}.

\subsection{Essential Matrix Estimation and Decomposition}
As we have described in the previous section, $E$ is estimated 
by the function {\tt estimateEssentialMatrix} of the MATLAB's 
Computer Vision Toolbox.
We use both frontal and rear image points for this estimation because
more correspondences between image pairs produces more accurate results.

Once $E$ is estimated, we need to decompose it in the 
\( [R|\myvec{t} ] \) form. This is again performed by a Computer Vision 
Toolbox's function; i.e., {\tt relativeCameraPose}.
The inputs for this function are $E$, the camera parameters, and matches found in the last two images.
We need these matches because there are four possible SVD decompositions for $E$. Each of these represents a different physical configuration for the cameras and world points.
In order to decide which decomposition is correct, the {\tt relativeCameraPose}
reprojects the matches in their corresponding world points according to each 
decomposition and selects the one that reprojects most of the correspondences in 
front of both cameras.
Since our cameras are full spherical and the image points belong to the 
rear hemisphere too, we need to provide only those points that belong to the 
frontal hemisphere as input to this function. In this way, the routine 
can correctly estimate the camera's positions relative to the set of frontal
points just like it would do for perspective cameras.
The reduced number of matching points for the input of the 
{\tt relativeCameraPose} function does not compromise the accuracy of the pose 
estimated, since those matches are used only to choose the 
correct SVD.

\subsection{Relative Scale Estimation}
Every relative motion between two views can only be estimated up to an unknown 
scale factor. Indeed the scale affects just the translation, but it has to 
be computed in order to create a coherent set of camera poses.
We obtain the relative scale using 
Equation~\ref{eq:relative_scale} \cite{scaramuzzaVisualOdometryI}.
This equation provides as many results as 3D points present in the last three
frames. In order to compensate for the effects of outliers, we take the 
median.

\subsection{Motion Composition}
Once we have estimated the relative motion between two views, we use 
Equation~\ref{eq:motion_composition} to compute the new camera pose $C_n$ from 
the last estimated pose $C_{n-1}$ and the results $R_{n}$ and $t_n$ obtained 
from the decomposition of $E$.
We set the first orientation, $R_0$, to $I$ and the magnitude of the first 
translation, $t_0$, to 1.

\subsection{Windowed Bundle Adjustment}
Since every local motion is inevitably affected by error, the overall 
camera's path estimation tends to deviate from the real trajectory.
In order to reduce the drift and to get closer to a better starting point 
for the final bundle adjustment, we perform a bundle adjustment over the last five poses
every time we process a new frame.
This local optimization step on the most recent subset of frames is called
\textit{Windowed Bundle Adjustment}.

The bundle adjustment tries to reduce the sum of reprojection errors by changing the
world points, and camera positions. 
We keep the camera poses associated with the two oldest frames of the window 
fixed in order to prevent the adjustment from modifying the reconstruction's 
scale. This constraint also helps reducing the number of variables for the 
adjustment.

\subsection{Global Bundle Adjustment}
When all the views have been processed, we perform a final bundle adjustment 
step, in order to further reduce drift. Again, the first two poses are fixed in 
order to keep the relative scale.

\section{Point Cloud Densification}
\label{sec:pipeline_densification}
\todo[inline]{ancora da scrivere}
