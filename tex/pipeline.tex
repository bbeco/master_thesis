\chapter{Spherical SFM}
\todo[inline]{aggiungere una sessione in cui si descrive il sistema di riferimento e 
altre convenzioni}
\section{Overview}
We now describe the pipeline we designed to create a dense point cloud from 
a set of equirectangular images.

The pipeline is composed of two main parts: an initial camera poses estimation
step followed by a densification phase. The former aims to reconstruct the 
camera trajectory, which means to estimate the camera position for each frame 
considered. The latter one relies on the previous result to create a dense 
3D point cloud that represents a simple reconstruction of the environment.
The Figure (...) is a simple visualization of this two macro parts of the 
pipeline.

The camera poses estimation phase is similar to other classical visual 
odometry pipeline; the real differences lie in the variables format and in 
the details concerning the procedures used. For each new frame 
(in the equirectangular format), we locate the
SURF keypoints; we consider those keypoints whose inclination angle belongs 
to the range [-60\degree; +60\degree] because the poles are affected by great 
distortions, thus robust matches outside this interval are rare.
Then we look for matches in the last two frames and a filter performs some 
statystical analysis on the correspondances found and decides whether the frame 
is suitable for a robust pose estimation or not (in which case the frame is 
discarded).

If the frame is kept, its matches with the previous one are converted
and used for the essential matrix estimation. Once E has been obtained, 
it can be decomposed in the \([R|t]\) form where R is a rotation matrix 
and t is a translation vector up to an unknown scale factor.

The relative scale can be estimated thanks to the world points which have been 
triangulated thorough the matches and frame considered in previous pipeline 
iteration.

The pose for each camera can then be described as the composition of the 
motions between each view pair. We perform a bundle adjustment for the last 5 
poses in order to reduce the effect of drift.

Once every image has been processed by the visual odometry pipeline, a final 
bundle adjustment is run. It optimizes every camera pose estimated in order to 
further reduce drift error.

In the following sections we describe every detail of the steps in our pipeline.

\section{Key Points and Features extraction}
In this step, we extract SURF key points and their descriptor out of 
equirectangular images. SURF 
interest points are rotation and scale invariant just like SIFT. But unlike 
the latter, SURF detection is faster and there is no patent for it. 
This makes SURF implementation available in most computer vision frameworks and 
libraries.

\section{Key points filtering}
The equirectangular format for spherical images introduces significant 
distortions in the areas around the poles. Interesting points in those areas 
are unlikely to match reliably with other points in a consecutive view, 
so they are just discarded.

Besides, in our experiments setup, the north and south poles usually point 
upward and downward respectively while most of the matches useful for pose 
estimation comes from the sides of the camera, so the removal of interesting 
points near the poles does not really affect the final result.

\section{Feature matching}
The matching strategy we carried out for equirectangular images is the same one 
adopted with standard images: two key points matches if the distance between 
their descriptors is less than a given threshold. Ambiguous matches 
are discarded if the ratio between the distances to the two closest matches is 
above a maximum.

We enforce matching robustness by forcing unique matching, that is: a feature 
in an image can possibly match to one feature only in the other image.
This is obtained with a first former forward matching followed by a backward 
pass that, in case of multiple matches, selects the best one only.

\section{Matches filter}
In this step of the pipeline the matches are analyzed in order to decide 
whether a frame is kept because it is useful for motion estimation or 
it has to be discarded.

We assume the environment considered for our studies is static: no people, 
cars or any kind of moving object is present in our scene. In this case the 
apparent movement of corresponding objects in different views is caused by 
the camera motion only. This hypothesis simplifies the pipeline and, 
in particular, provides some hints to the design of this filter.

This filter compute the disparity between every correspondence found in the 
two views. It compares the median of the 20\% of the matches with the highest 
disparity value with a threshold. If such a median is above 
the threshold, the frame is kept for further processing in the pipeline, 
otherwise it is discarded.

The reason why we consider the correspondances with greatest disparity is 
because, when the camera rotation is limited, the points that moves very little 
in consecutive views are usually far from the camera and they are less
useful than closer points to estimate camera motion. 

\section{3D-to-2D Key points conversion}
This step converts the key point format of equirectangular images to a new one 
suitable for the essential matrix estimation routine.
First the latitude and longitude coordinates of each feature point are 
extracted from the 2D mapping according to the following formula.
\begin{align}
\label{eq:llConversion}
\theta &= \frac{u}{W 2 \pi} - \pi \\
\phi &= \frac{\pi}{2} - \frac{v}{H \pi}
\end{align}
Where \textit{W} and \textit{H} are the image width and height respectively.

Once we have the spherical coordinates, we can use them to obtain a set of 3D 
key points with cartesian coordinates. In order to estimate the essential 
matrix for each pairs of views, we use the equation introduced by 
Longuet-Higgins in \cite{longuet1981computer}:

\begin{equation}
\label{eq:essentialMatrix}
\mathbf{p}'^\top E\mathbf{p} = 0
\end{equation}

Where 
\begin{math}\mathbf{p} = K^{-1} \mathbf{m},
\mathbf{p}' = K^{-1} \mathbf{m}'\end{math} are the normalized coordinates of 
two matching points, \begin{math}\mathbf{m}\end{math} and
\begin{math}\mathbf{m'}\end{math}, in the two images of the pair.
The point coordinates we obtain from \ref{eq:llConversion} and the consecutive 
transformation to cartesian do not need any normalization since they are not 
affected by any intrinsic parameter
(\begin{math}K = I\end{math}).

In order to exploit the routines to estimate the essential matrix available 
in the Matlab's Computer Vision Toolbox, we have to do an additional simple 
step.
Indeed this toolbox's routines deals with 2D images and, thus, they expect 
2D vectors when the input arguments are image points, however our camera
provides 3D image points.

We noticed that we can multiply the points \textbf{\textit{p}}
and \textbf{\textit{p}}' by two 
scalars \begin{math}{\lambda}\end{math} and 
\begin{math}{\lambda}'\end{math} and the equation is still valid.

\begin{equation*}
\lambda'\mathbf{p}'^\top E\lambda\mathbf{p} = 0
\end{equation*}

So, we divide the 3D points obtained from the spherical images by their 
3rd component, discard it, and use the resulted 2D points as input for the 
Matlab's essential matrix estimation routine.

\section{Essential matrix estimation and decomposition}
As we described in the previous section, the essential matrix is estimated 
by the function {\tt estimateEssentialMatrix} of the Computer Vision Toolbox.
We use both frontal and backward image points for this estimation because
the increased number of correspondances between image pair produces 
a more accurate result.

Once \textit{E} has been estimated, we need to decompose it in the 
\( [R|\mathbf{t} ] \) form. This is again performed by a Computer Vision 
Toolbox's function, {\tt relativeCameraPose}.
The input for this function are: the essential matrix computed before, the 
camera parameter structure and two sets of matching image points of the two 
images.
The reason why this function needs the matches is because there are 4 possible 
SVD decomposition for the essential matrix. Each one of this form represents a 
different physical configuration for the cameras and world points.
In order to decide which decomposition is correct, the {\tt relativeCameraPose}
reproject the matches in their corresponding world points according to each 
decomposition and select the one that reprojects most of the correspondances in 
front of both cameras.

Since our cameras are full spherical and the image points belong to the 
backwards hemisphere too, we need to select only those matches whose points 
belong to the frontal hemisphere in both cameras.

The reduced number of matching points for the input of the 
{\tt relativeCameraPose} function does not compromise the accuracy of the pose 
estimated, since those matches are used only to choose the 
correct SVD.

\section{Relative scale estimation}
Every relative motion between two views can only be estimated up to an unknown 
scale factor. Indeed the scale affects the translation only but still it has to 
be computed in order to create a coherent set of camera poses.

We obtain the relative scale with the formula from 
\cite{scaramuzzaVisualOdometryI}:
\begin{equation}
r = \frac{\| X_{k-1, i} - X_{k - 1, j} \|}
	{\| X_{k, i} - X_{k, j} \|}
\end{equation}
